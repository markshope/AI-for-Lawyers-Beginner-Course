{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cross-Lingual Similarity with TF-Hub Multilingual Universal Encoder",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markshope/AI-for-Lawyers-Beginner-Course/blob/master/Cross_Lingual_Similarity_with_TF_Hub_Multilingual_Universal_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "co7MV6sX7Xto"
      },
      "source": [
        "# Cross-Lingual Similarity with Multilingual Universal Sentence Encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eAVQGidpL8v5"
      },
      "source": [
        "This notebook illustrates how to access the Multilingual Universal Sentence Encoder module and use it for sentence similarity in Chinese and English. This module is an extension of the [original Universal Encoder module](https://tfhub.dev/google/universal-sentence-encoder/2).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOTzp8O36CyQ"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "This section sets up the environment for access to the Multilingual Universal Sentence Encoder Module\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "lVjNK8shFKOC",
        "colab": {}
      },
      "source": [
        "#@title Setup Environment\n",
        "!pip uninstall --quiet --yes tensorflow\n",
        "!pip install --quiet tensorflow-gpu==1.13.1\n",
        "!pip install --quiet tensorflow-hub\n",
        "!pip install --quiet bokeh\n",
        "!pip install --quiet tf-sentencepiece\n",
        "!pip install --quiet simpleneighbors\n",
        "!pip install --quiet tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MSeY-MUQo2Ha",
        "colab": {}
      },
      "source": [
        "#@title Setup common imports and functions\n",
        "import bokeh\n",
        "import bokeh.models\n",
        "import bokeh.plotting\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
        "import sklearn.metrics.pairwise\n",
        "\n",
        "from simpleneighbors import SimpleNeighbors\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "def visualize_similarity(embeddings_1, embeddings_2, labels_1, labels_2,\n",
        "                         plot_title,\n",
        "                         plot_width=1200, plot_height=600,\n",
        "                         xaxis_font_size='12pt', yaxis_font_size='12pt'):\n",
        "\n",
        "  assert len(embeddings_1) == len(labels_1)\n",
        "  assert len(embeddings_2) == len(labels_2)\n",
        "\n",
        "  # arccos based text similarity (Yang et al. 2019; Cer et al. 2019)\n",
        "  sim = 1 - np.arccos(\n",
        "      sklearn.metrics.pairwise.cosine_similarity(embeddings_1,\n",
        "                                                 embeddings_2))/np.pi\n",
        "\n",
        "  embeddings_1_col, embeddings_2_col, sim_col = [], [], []\n",
        "  for i in range(len(embeddings_1)):\n",
        "    for j in range(len(embeddings_2)):\n",
        "      embeddings_1_col.append(labels_1[i])\n",
        "      embeddings_2_col.append(labels_2[j])\n",
        "      sim_col.append(sim[i][j])\n",
        "  df = pd.DataFrame(zip(embeddings_1_col, embeddings_2_col, sim_col),\n",
        "                    columns=['embeddings_1', 'embeddings_2', 'sim'])\n",
        "\n",
        "  mapper = bokeh.models.LinearColorMapper(\n",
        "      palette=[*reversed(bokeh.palettes.YlOrRd[9])], low=df.sim.min(),\n",
        "      high=df.sim.max())\n",
        "\n",
        "  p = bokeh.plotting.figure(title=plot_title, x_range=labels_1,\n",
        "                            x_axis_location=\"above\",\n",
        "                            y_range=[*reversed(labels_2)],\n",
        "                            plot_width=plot_width, plot_height=plot_height,\n",
        "                            tools=\"save\",toolbar_location='below', tooltips=[\n",
        "                                ('pair', '@embeddings_1 ||| @embeddings_2'),\n",
        "                                ('sim', '@sim')])\n",
        "  p.rect(x=\"embeddings_1\", y=\"embeddings_2\", width=1, height=1, source=df,\n",
        "         fill_color={'field': 'sim', 'transform': mapper}, line_color=None)\n",
        "\n",
        "  p.title.text_font_size = '12pt'\n",
        "  p.axis.axis_line_color = None\n",
        "  p.axis.major_tick_line_color = None\n",
        "  p.axis.major_label_standoff = 16\n",
        "  p.xaxis.major_label_text_font_size = xaxis_font_size\n",
        "  p.xaxis.major_label_orientation = 0.25 * np.pi\n",
        "  p.yaxis.major_label_text_font_size = yaxis_font_size\n",
        "  p.min_border_right = 300\n",
        "\n",
        "  bokeh.io.output_notebook()\n",
        "  bokeh.io.show(p)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gk2IRjZFGDsK"
      },
      "source": [
        "This is additional boilerplate code where we import the pre-trained ML model we will use to encode text throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mkmF3w8WGLcM",
        "colab": {}
      },
      "source": [
        "#@title Import Multilingual Module\n",
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/1'\n",
        "\n",
        "# Set up graph.\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  multiling_embed = hub.Module(module_url)\n",
        "  embedded_text = multiling_embed(text_input)\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n",
        "\n",
        "# Initialize session.\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jhLPq6AROyFk"
      },
      "source": [
        "# Visualize Text Similarity Between Chinese and English\n",
        "With the sentence embeddings now in hand, we can visualize semantic similarity between Chinese and English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8xdAogbxJDTD"
      },
      "source": [
        "## Computing Text Embeddings\n",
        "\n",
        "We first define a set of sentences in English and Chinese. Then, we precompute the embeddings for our sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q8F4LNGFqOiq",
        "colab": {}
      },
      "source": [
        "# Force Majeure and Compliance Clause.\n",
        "\n",
        "chinese_sentences = ['賣方和買方應遵守適用於本合約的法律。', '被告可以通過證明損失係由原告自己或不可抗力因素造成而免除自己的責任。']\n",
        "english_sentences = ['Buyer shall comply with all applicable laws, regulations, and ordinances.', 'Seller shall not be liable for any failure or delay in fulfilling or performing.']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "weXZqLtTJY9b",
        "colab": {}
      },
      "source": [
        "# Compute embeddings.\n",
        "\n",
        "en_result = session.run(embedded_text, feed_dict={text_input: english_sentences})\n",
        "zh_result = session.run(embedded_text, feed_dict={text_input: chinese_sentences})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_3zGWuF-GhUm"
      },
      "source": [
        "## Visualizing Similarity\n",
        "\n",
        "With text embeddings in hand, we can take their dot-product to visualize how similar sentences are between the languages. A darker color indicates the embeddings are semantically similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ueoRO8balwwr"
      },
      "source": [
        "### English-Chinese Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xA7anofVlxL7",
        "colab": {}
      },
      "source": [
        "visualize_similarity(en_result, zh_result, english_sentences, chinese_sentences, 'English-Chinese Clause Similarity')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shC_lyXgU9mV",
        "colab_type": "text"
      },
      "source": [
        "# Copyright Notices\n",
        "\n",
        "The following copyright statements and licenses apply to various open source software components (or portions thereof) that are included with the code in this file. The code does not necessarily use all the open source software components referred to below and may also only use portions of a given component."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RUymE2l9GZfO"
      },
      "source": [
        "**Copyright 2019 The TensorFlow Hub Authors.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "JMyTNwSJGGWg",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}